from __future__ import unicode_literals
import gym
import gym_gridworld

import optuna
import random
import openpyxl
import numpy as np
import pandas as pd
from datetime import datetime
from collections import deque
import matplotlib.pyplot as plt


import torch
import torch.nn as nn
import torch.optim as optim


#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

env = gym.make('gridworld-v0')


env.seed(100)
random.seed(100)
np.random.seed(100)
torch.manual_seed(100)

#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

class QNetwork(nn.Module):
    def __init__(self, input_size, output_size, hidden1, hidden2): # stateê°€ ì¸í’‹, ê·¸ì— ëŒ€í•œ Qê°’ ê³„ì‚°ì´ ì•„ì›ƒí’‹
        super(QNetwork, self).__init__()
        self.layer1 = nn.Sequential(
                                    nn.Linear(input_size, hidden1),
                                    nn.ReLU())
                                    
        self.layer2 = nn.Sequential(
                                    nn.Linear(hidden1, hidden2),
                                    nn.ReLU() )

        self.layer3 = nn.Sequential(    nn.Linear(hidden2, output_size)  )
    
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x) #            tensor([ 0.0757, -0.0513], grad_fn=) í˜•íƒœì˜ ì¶œë ¥ì´ ë‚˜ì˜´.
#                                        torch.float32
        return x


#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

class ReplayBuffer:

    # state, action, ë“± ë³„ë¡œ ì €ì¥ í•  ë¹ˆ array ìƒì„±
    def __init__(self, obs_dim, act_dim, buff_size):        # obs_dim=4, act_dim=1, size=100_000
        self.state_buff = np.zeros([buff_size, obs_dim])    # ì•„ë˜ì— np.zeros() ì„¤ëª… ì°¸ì¡° [64, 4]
        self.action_buff = np.zeros([buff_size, 1])
        self.reward_buff = np.zeros([buff_size, 1], dtype=np.float32)
        self.next_state_buff = np.zeros([buff_size, obs_dim])
        self.done_buff = np.zeros([buff_size, 1], dtype=np.float32)


        # self.stateì˜ ê²½ìš°, [64, 4]ì˜ í¬ê¸°ì¸ë°, state í•˜ë‚˜ëŠ” [0.3233, 2.3241, -0.3233, -2.3241]ì˜ í˜•íƒœë¡œ ìƒê²¼ìŒ.
        # ì € state í•˜ë‚˜ë¥¼ ì €ì¥í•˜ë©´, (0,0)ì— ì €ì¥ë˜ëŠ” ê²Œ ì•„ë‹ˆë¼, component ë³„ë¡œ í•œ í–‰ì— ë‹¤ ë“¤ì–´ê°.
        # ê·¸ë˜ì„œ 64 ìƒ˜í”Œì— ëŒ€í•´ 64ê°œ í–‰ì´ ìƒê¹€.
        
        self.ptr = 0 # ìƒˆ experienceê°€ ì €ì¥ë  ìœ„ì¹˜ë¥¼ ê°€ë¦¬í‚´.
        self.size = 0
        self.max_size = buff_size

    def store(self, state, action, reward, next_state, done): # np.ndarray í˜•íƒœë¡œ ì €ì¥

        self.state_buff[self.ptr] = state   # ì €ì¥ë˜ëŠ” ë°©ì‹ì€ ì•„ë˜ np.zeros() ì„¤ëª… ì°¸ì¡°
        self.action_buff[self.ptr] = action
        self.reward_buff[self.ptr] = reward
        self.next_state_buff[self.ptr] = next_state
        self.done_buff[self.ptr] = done
        
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def sample(self, batch_size):
        idx= np.random.randint(0, self.size, size=batch_size) # ëœë¤í•˜ê²Œ experienceë¥¼ ë½‘ì•„ì˜´

        return dict(state=torch.tensor(self.state_buff[idx], dtype=torch.float32), # NNì— ë„£ì„ ë•Œ torch.float32 í˜•ì´ì–´ì•¼ í•¨.
                    action=torch.tensor(self.action_buff[idx], dtype=torch.long),
                    reward=torch.tensor(self.reward_buff[idx], dtype=torch.float32),
                    next_state=torch.tensor(self.next_state_buff[idx], dtype=torch.float32),
                    done=torch.tensor(self.done_buff[idx]))  # ì‚¬ìš©í•  ë•ŒëŠ” keyë¡œ ì¸ë±ì‹± í•´ì„œ ë¶ˆëŸ¬ì˜¬ ê±°ì„.

#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

class Agent:
    def __init__(self, init_eps, min_eps, eps_decay, gamma, target_update_freq,
                qnet, target_qnet, optimizer, criterion):

        # epsëŠ” ì‚¬ì‹¤ attributeë¡œ ì•ˆ ë§Œë“¤ê³  'get_action'ì—ì„œ ë°”ë¡œ ì¨ë„ ë˜ëŠ”ë°
        # Hyperparameter Tuningí•  ê±°ë¼ì„œ ì´ë ‡ê²Œ ì”€
        self.init_eps = init_eps #0.9
        self.min_eps = min_eps #0.05
        self.eps = self.init_eps
        self.eps_decay = eps_decay
        self.timer = 0

        self.gamma = gamma
        self.target_update_freq = target_update_freq

        self.qnet = qnet
        self.target_qnet = target_qnet
        self.optimizer = optimizer
        self.criterion = criterion
        
    # actionì€ 0 ë˜ëŠ” 1ì˜ scalarë¡œ ë°˜í™˜ í•  ê±°ì„
    def get_action(self, state, step):
        self.timer += 1
        self.eps = self.min_eps + (self.init_eps - self.min_eps) * np.exp(-1. * self.timer / self.eps_decay)

        # ì²˜ìŒì—ëŠ” eps_thresholdê°€ í° ê°’ì´ë¼ elseì—ì„œë§Œ ì‘ë™í•¨
        if random.random() > self.eps:
            action = self.qnet(state).detach().squeeze().numpy().argmax(axis=0)
            return int(action)

        else:
            action = env.action_space.sample()
            return action



    def learn(self, batch, current_epi):
        state = batch['state'] # tensor([[-1.0468, -0.8232,  1.4239,  0.4460],
        #                                [-8.8516e-01,  4.3342e-02,  1.6168e+00, -7.7498e-01], .......


        action = batch['action'] # tensor([[1], 
        #                                  [0], 
        #                                  [0], 

        reward = batch['reward'] # tensor([[1.],
        #                                  [1.],
        #                                  [1.],

        next_state = batch['next_state'] # stateì™€ ê°™ìŒ

        done = batch['done'] #tensor([[0.],
        #                             [0.],
        #                             [0.],


        # Q(S,A)ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„.
        # Qê°’ì€ ANNìœ¼ë¡œ ì˜ˆì¸¡í•œ ê±´ë°, ê·¸ ì¤‘ì—ì„œ ì‹¤ì œë¡œ í–ˆë˜ actionì„ gather í™œìš©í•´ ì°¾ìŒ.
        # dim=1ì´ë‹ˆê¹Œ 0ì—´ê³¼ 1ì—´ ì¤‘ì—ì„œ indexì— ë§ëŠ” ê²ƒë“¤ì„ ê³¨ë¼ì˜¬ ê±°ì„
        current_q = self.qnet(state).gather(dim=1, index=action) # tensor([[0.3761],
        #                                                                  [0.3822],
        #                                                                  [0.3724],....... grad_fn=)

        # max Q(S',A')ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„.
        # ê° í–‰ë§ˆë‹¤ 0ê³¼ 1ì— ëŒ€í•˜ì—¬ Qê°’ì´ ë‚˜ì™”ëŠ”ë°, dim=1ì´ë‹ˆê¹Œ maxì— í•´ë‹¹í•˜ëŠ” ì—´ë§Œ ë½‘ì•„ì˜´
        next_q = self.target_qnet(next_state).max(dim=1)[0].reshape(-1, 1)#tensor([[0.3761],
        #                                                                          [0.3822],
        #                                                                          [0.3724],....... grad_fn=)

        # R + gamma * max Q(S',A')
        td_target = reward + self.gamma * next_q * (1-done)

        # td_error = td_target - current_q
        loss = self.criterion(current_q, td_target)

        self.optimizer.zero_grad()
        loss.backward()

        # Q(S,A) <---- Q(S,A) + alpha*[R + gamma*max Q(S',A') - Q(S,A)]
        self.optimizer.step()


        if current_epi % self.target_update_freq == 0:
            self.target_qnet.load_state_dict(self.qnet.state_dict())


#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

# ê°ì¢… ì„¤ì •
max_epi = 600
reward_per_epi = []  # ì—í”¼ì†Œë“œë§ˆë‹¤ ë³´ìƒê°’ ì €ì¥
step_per_epi_list = []  # ì—í”¼ì†Œë“œë§ˆë‹¤ ìŠ¤í… ìˆ˜ ì €ì¥

# Environment ê´€ë ¨
obs_dim = 9*16 #env.observation_space.shape[0]
act_dim = 5 #env.action_space.n

# Network ê´€ë ¨                                        ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256)# state ë„£ì–´ì„œ, ê° actionì— ëŒ€í•œ Qê°’ ì¶œë ¥
target_qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256)
target_qnet.load_state_dict(qnet.state_dict())

optimizer = optim.RAdam(qnet.parameters(), lr=1e-4) #ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
criterion = nn.MSELoss()

# Replay Buffer ê´€ë ¨
memory = ReplayBuffer(obs_dim, act_dim, buff_size=100_000)

# Agent ê´€ë ¨                                               ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
agent = Agent(init_eps=0.9, min_eps=0.05, eps_decay=200, gamma=0.95, target_update_freq=4,
                qnet=qnet, target_qnet=target_qnet, optimizer=optimizer, criterion=criterion)


#ğŸ”¥ğŸ”¥ê²œ ê´€ë ¨ ì„¤ì •ğŸ”¥ğŸ”¥
max_epi = 500
reward_per_epi_list = []  # ì—í”¼ì†Œë“œë§ˆë‹¤ ë³´ìƒê°’ ì €ì¥
step_per_epi_list = []  # ì—í”¼ì†Œë“œë§ˆë‹¤ ìŠ¤í… ìˆ˜ ì €ì¥


#ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨
# ì €ì¥í•  ë•ŒëŠ” np.arrayì´ê³  êº¼ë‚´ì˜¬ ë•ŒëŠ” torch.tensor

time_box = []

start_time = datetime.now().replace(microsecond=0) 

for episode in range(1, max_epi+1): # ì—í”¼ì†Œë“œ ë‹¨ìœ„

    time_time = 0
    # ë°©ê¸ˆ ë§‰ ë°›ì€ stateëŠ” numpy.ndarray
    state = env.reset() 
    state = np.ravel(state)


    done = False

    step_per_epi = 0   # ì—í”¼ì†Œë“œë§ˆë‹¤ step ì–¼ë§ˆë‚˜ ë²„í‹°ë‚˜ ì¶”ì í•  ê±°ì„
    reward_per_epi = 0 # ì—í”¼ì†Œë“œë§ˆë‹¤ cumulative rewardë¥¼ ì¶”ì í•  ê±°ì„


    while not done: # step ë‹¨ìœ„

        time_time += 1

        #env.render()

        # actionì€ Env.ì— ì§ì ‘ ì „ë‹¬í•  ê°’ì´ë¯€ë¡œ Normalizationí•˜ë©´ ì•ˆ ë¨
        action = agent.get_action(torch.tensor(state, dtype=torch.float32),  #   1   //// 
                                step=step_per_epi)

        #print(action)

        env_info = env.step(action)

        next_state = env_info[0]
        next_state = np.ravel(next_state)

        reward = env_info[1] - 1
        reward = reward
        #print(reward)

        done = env_info[2]

        memory.store(state, action, reward, next_state, done)


        if memory.size >= 200: #ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
            batch = memory.sample(batch_size=32) # êº¼ë‚´ì˜¨ batchëŠ” ì „ë¶€ tensor í˜•íƒœë¡œ êµ¬ì„±ë¨
            agent.learn(batch, current_epi=episode)

        reward_per_epi += reward
        step_per_epi += 1

        state = next_state 

    reward_per_epi_list.append(reward_per_epi) # ì—í”¼ì†Œë“œë§ˆë‹¤ ëë‚˜ê³  ìµœì¢… ì–»ì€ ëˆ„ì  reward ì €ì¥
    step_per_epi_list.append(step_per_epi) # ì—í”¼ì†Œë“œë§ˆë‹¤ ëë‚˜ê³  ìµœì¢… ì–»ì€ step ì €ì¥

    print(f'Elapsed_Time: {datetime.now().replace(microsecond=0) - start_time}')

    if episode % 20 == 0:
        print(f'Episode: {episode},  Avg.Rewards: {np.mean(reward_per_epi_list[-100:])},  Avg.Steps: {np.mean(step_per_epi_list[-100:])},\
            Epsilon: {agent.eps}')

    #if np.mean(reward_per_epi_list[-100:]) >= 2000:
    #    print(f'Environment solved in {episode} episodes!')
    #    break


env.close()

# plotting rewards/episode
fig = plt.figure(figsize=(12, 8)) 
plt.plot(reward_per_epi_list)
plt.xlabel('Episode')
plt.ylabel('Reward')
plt.title('DQN_Rewards/Episode')
plt.grid()
plt.show()


end_time = datetime.now().replace(microsecond=0)

print(f'Start_Time: {start_time}')
print(f'End_Time: {end_time}')
print(f'Total_Training_Time: {end_time - start_time}')







