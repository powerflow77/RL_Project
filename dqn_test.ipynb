{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==The Start=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import gym\n",
    "import gym_gridworld\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('gridworld-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden1, hidden2): # state가 인풋, 그에 대한 Q값 계산이 아웃풋\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                                    nn.Linear(input_size, hidden1),\n",
    "                                    nn.ReLU())\n",
    "                                    \n",
    "        self.layer2 = nn.Sequential(\n",
    "                                    nn.Linear(hidden1, hidden2),\n",
    "                                    nn.ReLU() )\n",
    "\n",
    "        self.layer3 = nn.Sequential(    nn.Linear(hidden2, output_size)  )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x) #            tensor([ 0.0757, -0.0513], grad_fn=) 형태의 출력이 나옴.\n",
    "#                                        torch.float32\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    # state, action, 등 별로 저장 할 빈 array 생성\n",
    "    def __init__(self, obs_dim, act_dim, buff_size):        # obs_dim=4, act_dim=1, size=100_000\n",
    "        self.state_buff = np.zeros([buff_size, obs_dim])    # 아래에 np.zeros() 설명 참조 [64, 4]\n",
    "        self.action_buff = np.zeros([buff_size, 1])\n",
    "        self.reward_buff = np.zeros([buff_size, 1], dtype=np.float32)\n",
    "        self.next_state_buff = np.zeros([buff_size, obs_dim])\n",
    "        self.done_buff = np.zeros([buff_size, 1], dtype=np.float32)\n",
    "\n",
    "\n",
    "        # self.state의 경우, [64, 4]의 크기인데, state 하나는 [0.3233, 2.3241, -0.3233, -2.3241]의 형태로 생겼음.\n",
    "        # 저 state 하나를 저장하면, (0,0)에 저장되는 게 아니라, component 별로 한 행에 다 들어감.\n",
    "        # 그래서 64 샘플에 대해 64개 행이 생김.\n",
    "        \n",
    "        self.ptr = 0 # 새 experience가 저장될 위치를 가리킴.\n",
    "        self.size = 0\n",
    "        self.max_size = buff_size\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done): # np.ndarray 형태로 저장\n",
    "\n",
    "        self.state_buff[self.ptr] = state   # 저장되는 방식은 아래 np.zeros() 설명 참조\n",
    "        self.action_buff[self.ptr] = action\n",
    "        self.reward_buff[self.ptr] = reward\n",
    "        self.next_state_buff[self.ptr] = next_state\n",
    "        self.done_buff[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx= np.random.randint(0, self.size, size=batch_size) # 랜덤하게 experience를 뽑아옴\n",
    "\n",
    "        return dict(state=torch.tensor(self.state_buff[idx], dtype=torch.float32), # NN에 넣을 때 torch.float32 형이어야 함.\n",
    "                    action=torch.tensor(self.action_buff[idx], dtype=torch.long),\n",
    "                    reward=torch.tensor(self.reward_buff[idx], dtype=torch.float32),\n",
    "                    next_state=torch.tensor(self.next_state_buff[idx], dtype=torch.float32),\n",
    "                    done=torch.tensor(self.done_buff[idx]))  # 사용할 때는 key로 인덱싱 해서 불러올 거임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, init_eps, min_eps, eps_decay, gamma, target_update_freq,\n",
    "                qnet, target_qnet, optimizer, criterion):\n",
    "\n",
    "        # eps는 사실 attribute로 안 만들고 'get_action'에서 바로 써도 되는데\n",
    "        # Hyperparameter Tuning할 거라서 이렇게 씀\n",
    "        self.init_eps = init_eps #0.9\n",
    "        self.min_eps = min_eps #0.05\n",
    "        self.eps = self.init_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.timer = 0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.qnet = qnet\n",
    "        self.target_qnet = target_qnet\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    # action은 0 또는 1의 scalar로 반환 할 거임\n",
    "    def get_action(self, state, step):\n",
    "        self.timer += 1\n",
    "        self.eps = self.min_eps + (self.init_eps - self.min_eps) * np.exp(-1. * self.timer / self.eps_decay)\n",
    "\n",
    "        # 처음에는 eps_threshold가 큰 값이라 else에서만 작동함\n",
    "        if random.random() > self.eps:\n",
    "            action = self.qnet(state).detach().squeeze().numpy().argmax(axis=0)\n",
    "            return int(action)\n",
    "\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, batch, current_epi):\n",
    "        state = batch['state'] # tensor([[-1.0468, -0.8232,  1.4239,  0.4460],\n",
    "        #                                [-8.8516e-01,  4.3342e-02,  1.6168e+00, -7.7498e-01], .......\n",
    "\n",
    "\n",
    "        action = batch['action'] # tensor([[1], \n",
    "        #                                  [0], \n",
    "        #                                  [0], \n",
    "\n",
    "        reward = batch['reward'] # tensor([[1.],\n",
    "        #                                  [1.],\n",
    "        #                                  [1.],\n",
    "\n",
    "        next_state = batch['next_state'] # state와 같음\n",
    "\n",
    "        done = batch['done'] #tensor([[0.],\n",
    "        #                             [0.],\n",
    "        #                             [0.],\n",
    "\n",
    "\n",
    "        # Q(S,A)에 해당하는 부분임.\n",
    "        # Q값은 ANN으로 예측한 건데, 그 중에서 실제로 했던 action을 gather 활용해 찾음.\n",
    "        # dim=1이니까 0열과 1열 중에서 index에 맞는 것들을 골라올 거임\n",
    "        current_q = self.qnet(state).gather(dim=1, index=action) # tensor([[0.3761],\n",
    "        #                                                                  [0.3822],\n",
    "        #                                                                  [0.3724],....... grad_fn=)\n",
    "\n",
    "        # max Q(S',A')에 해당하는 부분임.\n",
    "        # 각 행마다 0과 1에 대하여 Q값이 나왔는데, dim=1이니까 max에 해당하는 열만 뽑아옴\n",
    "        next_q = self.target_qnet(next_state).max(dim=1)[0].reshape(-1, 1)#tensor([[0.3761],\n",
    "        #                                                                          [0.3822],\n",
    "        #                                                                          [0.3724],....... grad_fn=)\n",
    "\n",
    "        # R + gamma * max Q(S',A')\n",
    "        td_target = reward + self.gamma * next_q * (1-done)\n",
    "\n",
    "        # td_error = td_target - current_q\n",
    "        loss = self.criterion(current_q, td_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Q(S,A) <---- Q(S,A) + alpha*[R + gamma*max Q(S',A') - Q(S,A)]\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        if current_epi % self.target_update_freq == 0:\n",
    "            self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각종 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 설정\n",
    "max_epi = 600\n",
    "reward_per_epi = []  # 에피소드마다 보상값 저장\n",
    "step_per_epi_list = []  # 에피소드마다 스텝 수 저장\n",
    "\n",
    "# Environment 관련\n",
    "obs_dim = 16*16 #env.observation_space.shape[0]\n",
    "act_dim = 5 #env.action_space.n\n",
    "\n",
    "# Network 관련                                        🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
    "qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256) # state 넣어서, 각 action에 대한 Q값 출력\n",
    "target_qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256)\n",
    "target_qnet.load_state_dict(qnet.state_dict())\n",
    "\n",
    "optimizer = optim.RAdam(qnet.parameters(), lr=1e-4) #🔥🔥🔥🔥🔥🔥🔥\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Replay Buffer 관련\n",
    "memory = ReplayBuffer(obs_dim, act_dim, buff_size=100_000)\n",
    "\n",
    "# Agent 관련                                               🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
    "agent = Agent(init_eps=0.9, min_eps=0.05, eps_decay=200, gamma=0.95, target_update_freq=4,\n",
    "                qnet=qnet, target_qnet=target_qnet, optimizer=optimizer, criterion=criterion)\n",
    "\n",
    "\n",
    "#🔥🔥겜 관련 설정🔥🔥\n",
    "max_epi = 500\n",
    "reward_per_epi_list = []  # 에피소드마다 보상값 저장\n",
    "step_per_epi_list = []  # 에피소드마다 스텝 수 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' state 형태 ---> (16, 16),  <class 'numpy.ndarray'>\n",
    "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 2 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1, max_epi+1): # 에피소드 단위\n",
    "\n",
    "    # 방금 막 받은 state는 numpy.ndarray\n",
    "    state = env.reset() \n",
    "    state = np.ravel(state)\n",
    "\n",
    "\n",
    "    done = False\n",
    "\n",
    "    step_per_epi = 0   # 에피소드마다 step 얼마나 버티나 추적할 거임\n",
    "    reward_per_epi = 0 # 에피소드마다 cumulative reward를 추적할 거임\n",
    "\n",
    "    while not done: # step 단위\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        # action은 Env.에 직접 전달할 값이므로 Normalization하면 안 됨\n",
    "        action = agent.get_action(torch.tensor(state, dtype=torch.float32),  #   1   //// \n",
    "                                step=step_per_epi)\n",
    "\n",
    "        print(action)\n",
    "\n",
    "        env_info = env.step(action)\n",
    "\n",
    "        next_state = env_info[0]\n",
    "        next_state = np.ravel(next_state)\n",
    "\n",
    "        reward = env_info[1] # 1.0 //// \n",
    "\n",
    "        done = env_info[2]\n",
    "\n",
    "        memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        if memory.size >= 200: #🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
    "            batch = memory.sample(batch_size=32) # 꺼내온 batch는 전부 tensor 형태로 구성됨\n",
    "            agent.learn(batch, current_epi=episode)\n",
    "\n",
    "        reward_per_epi += reward\n",
    "        step_per_epi += 1\n",
    "\n",
    "        state = next_state \n",
    "\n",
    "\n",
    "    reward_per_epi_list.append(reward_per_epi) # 에피소드마다 끝나고 최종 얻은 누적 reward 저장\n",
    "    step_per_epi_list.append(step_per_epi) # 에피소드마다 끝나고 최종 얻은 step 저장\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        print(f'Episode: {episode},  Avg.Rewards: {np.mean(reward_per_epi_list[-100:])},  Avg.Steps: {np.mean(step_per_epi_list[-100:])},\\\n",
    "            Epsilon: {agent.eps}')\n",
    "\n",
    "    #if np.mean(reward_per_epi_list[-100:]) >= 2000:\n",
    "    #    print(f'Environment solved in {episode} episodes!')\n",
    "    #    break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==The End=="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
