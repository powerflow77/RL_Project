{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==The Start=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import gym\n",
    "import gym_gridworld\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('gridworld-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden1, hidden2): # stateÍ∞Ä Ïù∏Ìíã, Í∑∏Ïóê ÎåÄÌïú QÍ∞í Í≥ÑÏÇ∞Ïù¥ ÏïÑÏõÉÌíã\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                                    nn.Linear(input_size, hidden1),\n",
    "                                    nn.ReLU())\n",
    "                                    \n",
    "        self.layer2 = nn.Sequential(\n",
    "                                    nn.Linear(hidden1, hidden2),\n",
    "                                    nn.ReLU() )\n",
    "\n",
    "        self.layer3 = nn.Sequential(    nn.Linear(hidden2, output_size)  )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x) #            tensor([ 0.0757, -0.0513], grad_fn=) ÌòïÌÉúÏùò Ï∂úÎ†•Ïù¥ ÎÇòÏò¥.\n",
    "#                                        torch.float32\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    # state, action, Îì± Î≥ÑÎ°ú Ï†ÄÏû• Ìï† Îπà array ÏÉùÏÑ±\n",
    "    def __init__(self, obs_dim, act_dim, buff_size):        # obs_dim=4, act_dim=1, size=100_000\n",
    "        self.state_buff = np.zeros([buff_size, obs_dim])    # ÏïÑÎûòÏóê np.zeros() ÏÑ§Î™Ö Ï∞∏Ï°∞ [64, 4]\n",
    "        self.action_buff = np.zeros([buff_size, 1])\n",
    "        self.reward_buff = np.zeros([buff_size, 1], dtype=np.float32)\n",
    "        self.next_state_buff = np.zeros([buff_size, obs_dim])\n",
    "        self.done_buff = np.zeros([buff_size, 1], dtype=np.float32)\n",
    "\n",
    "\n",
    "        # self.stateÏùò Í≤ΩÏö∞, [64, 4]Ïùò ÌÅ¨Í∏∞Ïù∏Îç∞, state ÌïòÎÇòÎäî [0.3233, 2.3241, -0.3233, -2.3241]Ïùò ÌòïÌÉúÎ°ú ÏÉùÍ≤ºÏùå.\n",
    "        # Ï†Ä state ÌïòÎÇòÎ•º Ï†ÄÏû•ÌïòÎ©¥, (0,0)Ïóê Ï†ÄÏû•ÎêòÎäî Í≤å ÏïÑÎãàÎùº, component Î≥ÑÎ°ú Ìïú ÌñâÏóê Îã§ Îì§Ïñ¥Í∞ê.\n",
    "        # Í∑∏ÎûòÏÑú 64 ÏÉòÌîåÏóê ÎåÄÌï¥ 64Í∞ú ÌñâÏù¥ ÏÉùÍπÄ.\n",
    "        \n",
    "        self.ptr = 0 # ÏÉà experienceÍ∞Ä Ï†ÄÏû•Îê† ÏúÑÏπòÎ•º Í∞ÄÎ¶¨ÌÇ¥.\n",
    "        self.size = 0\n",
    "        self.max_size = buff_size\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done): # np.ndarray ÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
    "\n",
    "        self.state_buff[self.ptr] = state   # Ï†ÄÏû•ÎêòÎäî Î∞©ÏãùÏùÄ ÏïÑÎûò np.zeros() ÏÑ§Î™Ö Ï∞∏Ï°∞\n",
    "        self.action_buff[self.ptr] = action\n",
    "        self.reward_buff[self.ptr] = reward\n",
    "        self.next_state_buff[self.ptr] = next_state\n",
    "        self.done_buff[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx= np.random.randint(0, self.size, size=batch_size) # ÎûúÎç§ÌïòÍ≤å experienceÎ•º ÎΩëÏïÑÏò¥\n",
    "\n",
    "        return dict(state=torch.tensor(self.state_buff[idx], dtype=torch.float32), # NNÏóê ÎÑ£ÏùÑ Îïå torch.float32 ÌòïÏù¥Ïñ¥Ïïº Ìï®.\n",
    "                    action=torch.tensor(self.action_buff[idx], dtype=torch.long),\n",
    "                    reward=torch.tensor(self.reward_buff[idx], dtype=torch.float32),\n",
    "                    next_state=torch.tensor(self.next_state_buff[idx], dtype=torch.float32),\n",
    "                    done=torch.tensor(self.done_buff[idx]))  # ÏÇ¨Ïö©Ìï† ÎïåÎäî keyÎ°ú Ïù∏Îç±Ïã± Ìï¥ÏÑú Î∂àÎü¨Ïò¨ Í±∞ÏûÑ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, init_eps, min_eps, eps_decay, gamma, target_update_freq,\n",
    "                qnet, target_qnet, optimizer, criterion):\n",
    "\n",
    "        # epsÎäî ÏÇ¨Ïã§ attributeÎ°ú Ïïà ÎßåÎì§Í≥† 'get_action'ÏóêÏÑú Î∞îÎ°ú Ïç®ÎèÑ ÎêòÎäîÎç∞\n",
    "        # Hyperparameter TuningÌï† Í±∞ÎùºÏÑú Ïù¥Î†áÍ≤å ÏîÄ\n",
    "        self.init_eps = init_eps #0.9\n",
    "        self.min_eps = min_eps #0.05\n",
    "        self.eps = self.init_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.timer = 0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.qnet = qnet\n",
    "        self.target_qnet = target_qnet\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    # actionÏùÄ 0 ÎòêÎäî 1Ïùò scalarÎ°ú Î∞òÌôò Ìï† Í±∞ÏûÑ\n",
    "    def get_action(self, state, step):\n",
    "        self.timer += 1\n",
    "        self.eps = self.min_eps + (self.init_eps - self.min_eps) * np.exp(-1. * self.timer / self.eps_decay)\n",
    "\n",
    "        # Ï≤òÏùåÏóêÎäî eps_thresholdÍ∞Ä ÌÅ∞ Í∞íÏù¥Îùº elseÏóêÏÑúÎßå ÏûëÎèôÌï®\n",
    "        if random.random() > self.eps:\n",
    "            action = self.qnet(state).detach().squeeze().numpy().argmax(axis=0)\n",
    "            return int(action)\n",
    "\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, batch, current_epi):\n",
    "        state = batch['state'] # tensor([[-1.0468, -0.8232,  1.4239,  0.4460],\n",
    "        #                                [-8.8516e-01,  4.3342e-02,  1.6168e+00, -7.7498e-01], .......\n",
    "\n",
    "\n",
    "        action = batch['action'] # tensor([[1], \n",
    "        #                                  [0], \n",
    "        #                                  [0], \n",
    "\n",
    "        reward = batch['reward'] # tensor([[1.],\n",
    "        #                                  [1.],\n",
    "        #                                  [1.],\n",
    "\n",
    "        next_state = batch['next_state'] # stateÏôÄ Í∞ôÏùå\n",
    "\n",
    "        done = batch['done'] #tensor([[0.],\n",
    "        #                             [0.],\n",
    "        #                             [0.],\n",
    "\n",
    "\n",
    "        # Q(S,A)Ïóê Ìï¥ÎãπÌïòÎäî Î∂ÄÎ∂ÑÏûÑ.\n",
    "        # QÍ∞íÏùÄ ANNÏúºÎ°ú ÏòàÏ∏°Ìïú Í±¥Îç∞, Í∑∏ Ï§ëÏóêÏÑú Ïã§Ï†úÎ°ú ÌñàÎçò actionÏùÑ gather ÌôúÏö©Ìï¥ Ï∞æÏùå.\n",
    "        # dim=1Ïù¥ÎãàÍπå 0Ïó¥Í≥º 1Ïó¥ Ï§ëÏóêÏÑú indexÏóê ÎßûÎäî Í≤ÉÎì§ÏùÑ Í≥®ÎùºÏò¨ Í±∞ÏûÑ\n",
    "        current_q = self.qnet(state).gather(dim=1, index=action) # tensor([[0.3761],\n",
    "        #                                                                  [0.3822],\n",
    "        #                                                                  [0.3724],....... grad_fn=)\n",
    "\n",
    "        # max Q(S',A')Ïóê Ìï¥ÎãπÌïòÎäî Î∂ÄÎ∂ÑÏûÑ.\n",
    "        # Í∞Å ÌñâÎßàÎã§ 0Í≥º 1Ïóê ÎåÄÌïòÏó¨ QÍ∞íÏù¥ ÎÇòÏôîÎäîÎç∞, dim=1Ïù¥ÎãàÍπå maxÏóê Ìï¥ÎãπÌïòÎäî Ïó¥Îßå ÎΩëÏïÑÏò¥\n",
    "        next_q = self.target_qnet(next_state).max(dim=1)[0].reshape(-1, 1)#tensor([[0.3761],\n",
    "        #                                                                          [0.3822],\n",
    "        #                                                                          [0.3724],....... grad_fn=)\n",
    "\n",
    "        # R + gamma * max Q(S',A')\n",
    "        td_target = reward + self.gamma * next_q * (1-done)\n",
    "\n",
    "        # td_error = td_target - current_q\n",
    "        loss = self.criterion(current_q, td_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Q(S,A) <---- Q(S,A) + alpha*[R + gamma*max Q(S',A') - Q(S,A)]\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        if current_epi % self.target_update_freq == 0:\n",
    "            self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Í∞ÅÏ¢Ö ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞ÅÏ¢Ö ÏÑ§Ï†ï\n",
    "max_epi = 600\n",
    "reward_per_epi = []  # ÏóêÌîºÏÜåÎìúÎßàÎã§ Î≥¥ÏÉÅÍ∞í Ï†ÄÏû•\n",
    "step_per_epi_list = []  # ÏóêÌîºÏÜåÎìúÎßàÎã§ Ïä§ÌÖù Ïàò Ï†ÄÏû•\n",
    "\n",
    "# Environment Í¥ÄÎ†®\n",
    "obs_dim = 16*16 #env.observation_space.shape[0]\n",
    "act_dim = 5 #env.action_space.n\n",
    "\n",
    "# Network Í¥ÄÎ†®                                        üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•\n",
    "qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256) # state ÎÑ£Ïñ¥ÏÑú, Í∞Å actionÏóê ÎåÄÌïú QÍ∞í Ï∂úÎ†•\n",
    "target_qnet = QNetwork(input_size=obs_dim, output_size=act_dim, hidden1=256, hidden2=256)\n",
    "target_qnet.load_state_dict(qnet.state_dict())\n",
    "\n",
    "optimizer = optim.RAdam(qnet.parameters(), lr=1e-4) #üî•üî•üî•üî•üî•üî•üî•\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Replay Buffer Í¥ÄÎ†®\n",
    "memory = ReplayBuffer(obs_dim, act_dim, buff_size=100_000)\n",
    "\n",
    "# Agent Í¥ÄÎ†®                                               üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•\n",
    "agent = Agent(init_eps=0.9, min_eps=0.05, eps_decay=200, gamma=0.95, target_update_freq=4,\n",
    "                qnet=qnet, target_qnet=target_qnet, optimizer=optimizer, criterion=criterion)\n",
    "\n",
    "\n",
    "#üî•üî•Í≤ú Í¥ÄÎ†® ÏÑ§Ï†ïüî•üî•\n",
    "max_epi = 500\n",
    "reward_per_epi_list = []  # ÏóêÌîºÏÜåÎìúÎßàÎã§ Î≥¥ÏÉÅÍ∞í Ï†ÄÏû•\n",
    "step_per_epi_list = []  # ÏóêÌîºÏÜåÎìúÎßàÎã§ Ïä§ÌÖù Ïàò Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÌïôÏäµ ÏãúÏûë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' state ÌòïÌÉú ---> (16, 16),  <class 'numpy.ndarray'>\n",
    "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 2 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1]\n",
    " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
    " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
    " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1, max_epi+1): # ÏóêÌîºÏÜåÎìú Îã®ÏúÑ\n",
    "\n",
    "    # Î∞©Í∏à Îßâ Î∞õÏùÄ stateÎäî numpy.ndarray\n",
    "    state = env.reset() \n",
    "    state = np.ravel(state)\n",
    "\n",
    "\n",
    "    done = False\n",
    "\n",
    "    step_per_epi = 0   # ÏóêÌîºÏÜåÎìúÎßàÎã§ step ÏñºÎßàÎÇò Î≤ÑÌã∞ÎÇò Ï∂îÏ†ÅÌï† Í±∞ÏûÑ\n",
    "    reward_per_epi = 0 # ÏóêÌîºÏÜåÎìúÎßàÎã§ cumulative rewardÎ•º Ï∂îÏ†ÅÌï† Í±∞ÏûÑ\n",
    "\n",
    "    while not done: # step Îã®ÏúÑ\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        # actionÏùÄ Env.Ïóê ÏßÅÏ†ë Ï†ÑÎã¨Ìï† Í∞íÏù¥ÎØÄÎ°ú NormalizationÌïòÎ©¥ Ïïà Îê®\n",
    "        action = agent.get_action(torch.tensor(state, dtype=torch.float32),  #   1   //// \n",
    "                                step=step_per_epi)\n",
    "\n",
    "        print(action)\n",
    "\n",
    "        env_info = env.step(action)\n",
    "\n",
    "        next_state = env_info[0]\n",
    "        next_state = np.ravel(next_state)\n",
    "\n",
    "        reward = env_info[1] # 1.0 //// \n",
    "\n",
    "        done = env_info[2]\n",
    "\n",
    "        memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        if memory.size >= 200: #üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•\n",
    "            batch = memory.sample(batch_size=32) # Í∫ºÎÇ¥Ïò® batchÎäî Ï†ÑÎ∂Ä tensor ÌòïÌÉúÎ°ú Íµ¨ÏÑ±Îê®\n",
    "            agent.learn(batch, current_epi=episode)\n",
    "\n",
    "        reward_per_epi += reward\n",
    "        step_per_epi += 1\n",
    "\n",
    "        state = next_state \n",
    "\n",
    "\n",
    "    reward_per_epi_list.append(reward_per_epi) # ÏóêÌîºÏÜåÎìúÎßàÎã§ ÎÅùÎÇòÍ≥† ÏµúÏ¢Ö ÏñªÏùÄ ÎàÑÏ†Å reward Ï†ÄÏû•\n",
    "    step_per_epi_list.append(step_per_epi) # ÏóêÌîºÏÜåÎìúÎßàÎã§ ÎÅùÎÇòÍ≥† ÏµúÏ¢Ö ÏñªÏùÄ step Ï†ÄÏû•\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        print(f'Episode: {episode},  Avg.Rewards: {np.mean(reward_per_epi_list[-100:])},  Avg.Steps: {np.mean(step_per_epi_list[-100:])},\\\n",
    "            Epsilon: {agent.eps}')\n",
    "\n",
    "    #if np.mean(reward_per_epi_list[-100:]) >= 2000:\n",
    "    #    print(f'Environment solved in {episode} episodes!')\n",
    "    #    break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==The End=="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
