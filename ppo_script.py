from __future__ import unicode_literals
import gym
import gym_gridworld


import os
import glob # ê°ì¢… íŒŒì¼ ë‹¤ë£° ë•Œ ì‚¬ìš©.
import time
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from torch.distributions import MultivariateNormal

env = gym.make('gridworld-v0')

# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

print("============================================================================================")
# set device to cpu or cuda
device = torch.device('cpu')
if(torch.cuda.is_available()): 
    device = torch.device('cuda:0') 
    torch.cuda.empty_cache()
    print("Device set to : " + str(torch.cuda.get_device_name(device)))
else:
    print("Device set to : cpu")
print("============================================================================================")

# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

class RolloutBuffer:
    
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
    
    def clear(self):
        del self.actions[:]   # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”(ë¹„ìš°ê¸°)
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

#ğŸ”¥ Actorë‘ Criticì´ ê°™ì€ í´ë˜ìŠ¤ì— ìˆì§€ë§Œ, networkëŠ” ê°ê° ì‚¬ìš©í•¨.
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):
        super(ActorCritic, self).__init__()# state_dimì€ ê°€ì†ë„ ë“± 4, action_dim ì™¼ìª½ ì˜¤ë¥¸ìª½ 2

        self.has_continuous_action_space = has_continuous_action_space # True if action space is continuous.
        
        # continuous action spaceì˜ ê²½ìš°, ì´ˆê¸° Variance ê°’ í•„ìš”í•¨.
        if has_continuous_action_space: 
            self.action_dim = action_dim
            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)
        #                                (ì´ í¬ê¸° í…ì„œë¥¼)     ìœ„ ê°’ìœ¼ë¡œ ì±„ì›Œë¼.
        #                                ëŒ€ê°ì„ ì— ë”± ë¼ì›Œë„£ìœ¼ë ¤ë©´ (col_dim, )ì˜ ì°¨ì› ê°€ì ¸ì•¼í•¨. & std ì œê³±í•´ì„œ variance ë§Œë“¤ì—ˆìŒ.
        #                                [a, b, c, d] -> ì´ëŸ° í˜•ì‹ ê°€ì§€ê³  ë‚´ë ¤ê°€ì„œ Covariance Matrix ë§Œë“¤ ê±°ì„.

        #ğŸ”¥Actor
        #ğŸ”¥ğŸ”¥Continuous Action Space
        if has_continuous_action_space :
            self.actor = nn.Sequential(
                                    nn.Linear(state_dim, 64), 
                                    nn.Tanh(),
                                    nn.Linear(64, 64),
                                    nn.Tanh(),
                                    nn.Linear(64, action_dim) )
                                    
        #ğŸ”¥ğŸ”¥Discrete Action Space
        else:
            self.actor = nn.Sequential(
                                    nn.Linear(state_dim, 64),
                                    nn.Tanh(),
                                    nn.Linear(64, 64),
                                    nn.Tanh(),
                                    nn.Linear(64, action_dim),
                                    nn.Softmax(dim=-1)  )     # ê° actionì„ í•  í™•ë¥ ì´ ì¶œë ¥.

        #ğŸ”¥ Critic
        #ğŸ”¥ğŸ”¥ Criticì€ Actorê°€ continuousì¸ì§€ discreteì¸ì§€ ìƒê´€ì—†ìŒ.
        self.critic = nn.Sequential(
                                nn.Linear(state_dim, 64),   # self.actorì™€ self.criticì´ ì˜ˆì¸¡ì„. forward í•¨ìˆ˜ ì•ˆ ì”€.
                                nn.Tanh(),
                                nn.Linear(64, 64),
                                nn.Tanh(),
                                nn.Linear(64, 1)  )
    
    # ìœ„ì—ì„œëŠ” ì´ˆê¸° action_std ë„£ì–´ì¤€ ê±°ê³ , ì—¬ê¸°ì„œëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ë“¤ì–´ì˜¤ëŠ” ê²ƒë“¤ í™œìš©í•˜ëŠ” ê±°ì„.
    def set_action_std(self, new_action_std): 
        if self.has_continuous_action_space:
            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)
        else:
            print("--------------------------------------------------------------------------------------------")
            print("WARNING : Calling ActorCritic::set_action_std() on discrete action space policy")
            print("--------------------------------------------------------------------------------------------")

    def forward(self):
        raise NotImplementedError # ì˜ë„ì  ì—ëŸ¬ ë°œìƒ.
    
    # Environmentì— ìˆ˜í–‰í•  action ê°’ì„ ë½‘ì•„ë‚¼ ë•Œ ì“¸ ê±°ì„.
    def act(self, state):  # action ê°’ì´ë‘ ê·¸ë•Œì˜ logprobê°’ì„ ë°˜í™˜.

        #ğŸ”¥Continuous Action Space
        if self.has_continuous_action_space:
            action_mean = self.actor(state)                        # MeanğŸ”¥ğŸ”¥ì´ê²Œ ì˜ˆì¸¡ì„ forward í•¨ìˆ˜ ì•ˆ ì”€.ğŸ”¥ğŸ”¥
            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0) # ì´ˆê¸°ê°’ì´ë“  í˜„ì¬ê°’ì´ë“  ë°›ì•„ì„œ Covariance Matrix ë§Œë“¦.
            dist = MultivariateNormal(action_mean, cov_mat)        # Multivariate normal distribution ê³„ì‚°í•´ì¤Œ.
        
        #ğŸ”¥Discrete Action Space
        else:
            action_probs = self.actor(state)                       # ğŸ”¥ğŸ”¥Mean ì˜ˆì¸¡ğŸ”¥ğŸ”¥
            dist = Categorical(action_probs)                       # actionì— ëŒ€í•œ discrete distribution ê³„ì‚°í•´ì¤Œ.

        action = dist.sample()                                     # ìœ„ê°€ ë¬´ì—‡ì´ë“  ê°„ì—, action í•˜ë‚˜ ë½‘ì•„ì˜´.
        action_logprob = dist.log_prob(action)                     # ì´ë•Œì˜ log(p(action|state)) ê³„ì‚°
        
        return action.detach(), action_logprob.detach()
    
    # actëŠ” Environmentì— í•  action ë½‘ì•„ë‚´ê¸°
    # evaluateëŠ” Bufferì—ì„œ êº¼ë‚´ì˜¨ ë°ì´í„° ê°€ì§€ê³  ê³„ì‚°í•´ì£¼ëŠ” ê±°ì„.
    def evaluate(self, state, action):  # K_epoch Loop ì•ˆì—ì„œ í˜¸ì¶œë¨.
        
        #ğŸ”¥Continuous Action Space
        if self.has_continuous_action_space:
            action_mean = self.actor(state)                       # ğŸ”¥ğŸ”¥Mean ì˜ˆì¸¡ğŸ”¥ğŸ”¥
            
            action_var = self.action_var.expand_as(action_mean)   # í¬ê¸° ë§ê²Œ ì‚¬ì´ì¦ˆ í™•ì¥. & Variance
            cov_mat = torch.diag_embed(action_var).to(device)     # ì´ˆê¸°ê°’ì´ë“  í˜„ì¬ê°’ì´ë“  ë°›ì•„ì„œ Covariance Matrix ë§Œë“¦.
            dist = MultivariateNormal(action_mean, cov_mat)       # Multivariate normal distribution ê³„ì‚°í•´ì¤Œ.
            
            # For Single Action Environments.
            if self.action_dim == 1:
                action = action.reshape(-1, self.action_dim)
        
        #ğŸ”¥Discrete Action Space
        else:
            action_probs = self.actor(state)     # ğŸ”¥ì˜ˆì¸¡
            dist = Categorical(action_probs)
            
        action_logprobs = dist.log_prob(action)  
        dist_entropy = dist.entropy()            # ğŸ”¥ ë¶„í¬ì— ëŒ€í•œ ì—”íŠ¸ë¡œí”¼(ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ì— ì‚¬ìš©.)
        state_values = self.critic(state)        # ğŸ”¥ Value Function ê°’ ì˜ˆì¸¡ğŸ”¥ğŸ”¥
        
        return action_logprobs, state_values, dist_entropy


# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

class PPO:
    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma,\
                K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6): # ì´ˆê¸° action_stdê°’ ë„£ì–´ì¤Œ.

        self.has_continuous_action_space = has_continuous_action_space # continuous action spaceëƒ discreteëƒ

        #ğŸ”¥ğŸ”¥continuous action spaceì´ë©´ action_std_init ê°€ì ¸ë‹¤ ì“¸ê±°ê³ , discrete ë•ŒëŠ” None ë„£ì„ ê±°ì„.
        if has_continuous_action_space:
            self.action_std = action_std_init  

        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs
        
        # Replay Buffer ì§€ì •.
        self.buffer = RolloutBuffer() 

        #ğŸ”¥ğŸ”¥self.policy ì„ ì–¸ğŸ”¥ğŸ”¥
        # ì—…ë°ì´íŠ¸ í•  ë„¤íŠ¸ì›Œí¬ëŠ” self.policyì„.                                           ì´ˆê¸° action_std ë°›ìŒ.
        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)
        self.optimizer = optim.Adam([
                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},  
                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}  ])# í´ë˜ìŠ¤ ë‚´ ë‹¤ìˆ˜ì˜ NNì— ëŒ€í•˜ì—¬ ì“°ëŠ” í‘œí˜„.

        #ğŸ”¥ğŸ”¥self.policy_old ì„ ì–¸ğŸ”¥ğŸ”¥
        # self.policy_oldëŠ” weightë¥¼ ì „ë‹¬ë°›ê¸°ë§Œ í•¨.                                        ì´ê²ƒë„ ì´ˆê¸° action_std ë°›ìŒ.
        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)

        # ê°€ì§€ê³  ìˆë˜ NN weightsë¥¼ self.policy_oldì— ì´ì „í•´ë†“ìŒ.
        # ë‚˜ì¤‘ì— ratio ê³„ì‚°í•˜ë ¤ê³  í•˜ë‚˜ëŠ” ê³„ì† ì—…ë°ì´íŠ¸, ë‚˜ë¨¸ì§€ í•˜ë‚˜ëŠ” weight ë°›ê³  ê°€ë§Œíˆ ìˆìŒ.
        self.policy_old.load_state_dict(self.policy.state_dict())
        
        self.MseLoss = nn.MSELoss()

    def set_action_std(self, new_action_std): 
        if self.has_continuous_action_space:
            self.action_std = new_action_std
            self.policy.set_action_std(new_action_std)      # Covariance Matrixì— ì‚¬ìš©í•  variance ê°’ ê³„ì‚°
            self.policy_old.set_action_std(new_action_std)  # Covariance Matrixì— ì‚¬ìš©í•  variance ê°’ ê³„ì‚°.
        else:
            print("--------------------------------------------------------------------------------------------")
            print("WARNING : Calling PPO::set_action_std() on discrete action space policy")
            print("--------------------------------------------------------------------------------------------")

    
    def decay_action_std(self, action_std_decay_rate, min_action_std): 
        print("--------------------------------------------------------------------------------------------")
        if self.has_continuous_action_space:
            self.action_std = self.action_std - action_std_decay_rate
            self.action_std = round(self.action_std, 4)
            if (self.action_std <= min_action_std):
                self.action_std = min_action_std
                print("setting actor output action_std to min_action_std : ", self.action_std)
            else:
                print("setting actor output action_std to : ", self.action_std)
            self.set_action_std(self.action_std)

        else:
            print("WARNING : Calling PPO::decay_action_std() on discrete action space policy")
        print("--------------------------------------------------------------------------------------------")

    # Environmentì— ëŒ€í•´ ì–´ë–¤ actioní• ì§€ ì„ íƒí•¨.
    # Episode Loopì—ì„œ ëŒì•„ê°.
    def select_action(self, state):            # ì—í”¼ì†Œë“œì˜ ê³¼ì •ì—ì„œ, a ì„ íƒí•˜ê³ , ê·¸ì— ëŒ€í•œ log_prob êµ¬í•¨.
        #                                        ì§í›„ì—, s, a, log_prob ì €ì¥í•¨.     
        #ğŸ”¥Continuous Action Space
        if self.has_continuous_action_space:
            with torch.no_grad():
                state = torch.FloatTensor(state).to(device)
                action, action_logprob = self.policy_old.act(state)  # modelì˜ actê°€ ì˜ˆì¸¡í•´ì„œ ì´ëŸ° ê±° ë½‘ì•„ë‚´ëŠ” ê±°ì„.
            #                                                          env.ì— ìˆ˜í–‰ í•  ì¢‹ì€ actionì„ NNìœ¼ë¡œ ë½‘ì•„ë‚´ê² ìŒ.
            #                                                          
            self.buffer.states.append(state)             
            self.buffer.actions.append(action)
            self.buffer.logprobs.append(action_logprob)  # ê°ì¢… ê°’ ì €ì¥.

            return action.detach().cpu().numpy().flatten()
        
        #ğŸ”¥Discrete Action Space
        else:
            with torch.no_grad():
                state = torch.FloatTensor(state).to(device)
                action, action_logprob = self.policy_old.act(state)  # modelì˜ actê°€ ì˜ˆì¸¡í•´ì„œ ì´ëŸ° ê±° ë½‘ì•„ë‚´ëŠ” ê±°ì„.
            
            self.buffer.states.append(state)               
            self.buffer.actions.append(action)
            self.buffer.logprobs.append(action_logprob)    # ê°ì¢… ê°’ ì €ì¥.

            return action.item()


    #ğŸ”¥ğŸ”¥ğŸ”¥ K_epoch Loopì—ì„œ ì‹¤í–‰ë˜ëŠ” ê³¼ì •ì„.ğŸ”¥ğŸ”¥ğŸ”¥
    def update(self):
        # return Gtë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ê³¼ì •
        rewards = []
        discounted_reward = 0#          reversedëŠ” iterabelí•œ ê°ì²´ì— ëŒ€í•´ ìˆœì„œë¥¼ ë’¤ì§‘ìŒ.
        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)  # returns Gt ë§Œë“¤ì–´ê°.
            rewards.insert(0, discounted_reward)
            
        # Normalizing the rewards(PPO2 ë°˜ì˜)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        # convert list to tensor
        # Replay Bufferì— ì €ì¥ëœ ê²ƒë“¤ì„ ê°€ì ¸ë‹¤ê°€ ì—…ë°ì´íŠ¸ í•  ê±°ì„.
        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)
        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)
        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)

        #ğŸ”¥PPO ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ğŸ”¥---------------->> Training Loop ê°€ì¥ ë°”ê¹¥ìª½ì—ì„œ ëŒë¦¬ëŠ” ê±´ bufferì— ì €ì¥ë§Œ ì£¼êµ¬ì¥ì°½ í•˜ëŠ” ê±°ì„.
        #                                          íŠ¹ì • ë•Œë§ˆë‹¤ PPO ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸. í•™ìŠµ LoopëŠ” K_epochë§Œí¼ ëŒë¦¼. 
        for _ in range(self.K_epochs):

            # Evaluating old actions and values
            # ğŸ”¥í•™ìŠµ ëŒ€ìƒì€ self.policyì˜ ë„¤íŠ¸ì›Œí¬ë“¤ğŸ”¥
            # 'old' ì¨ìˆëŠ” ê²ƒë“¤ì€ Bufferì—ì„œ ê°€ì ¸ë‹¤ê°€ ì“°ëŠ” ê±°ì„.
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)

            # match state_values tensor dimensions with rewards tensor
            state_values = torch.squeeze(state_values)
            
            # Finding the ratio (pi_theta / pi_theta__old)
            ratios = torch.exp(logprobs - old_logprobs.detach())

            # Finding Surrogate Loss
            advantages = rewards - state_values.detach()   
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages

            #ğŸ”¥ğŸ”¥Loss Function(ë¶€í˜¸ ë§¤ìš° ì¤‘ìš”. pytorchëŠ” gradient descent ì œê³µ.)ğŸ”¥ğŸ”¥
            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy
            
            # í•™ìŠµ ìˆ˜í–‰.
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        # self.policyì— ëŒ€í•œ í•™ìŠµì´ ëë‚˜ë©´ self.policy_oldì— ë³µì‚¬í•´ì¤Œ.
        self.policy_old.load_state_dict(self.policy.state_dict())

        # clear buffer
        self.buffer.clear()
    
    def save(self, checkpoint_path):
        torch.save(self.policy_old.state_dict(), checkpoint_path)
   
    def load(self, checkpoint_path):
        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))
        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))



# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨


env_name = 'gridworld-v0'
has_continuous_action_space = False 

max_ep_len = 400                    # í•œ ê°œì˜ ì—í”¼ì†Œë“œ ë‹¹ ìµœëŒ€ 400 timestepê¹Œì§€ë§Œ~
#ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›ğŸ’›
max_training_timesteps = int(1e5)   #ğŸ’›ì „ì²´ timestepì€ 1e5ê¹Œì§€ë§Œ ---->> ì´ê±¸ë¡œ ì „ì²´ í•™ìŠµ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ëë‚¼ì§€ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.ğŸ’›

print_freq = max_ep_len * 4         # ì¶œë ¥ ì‹œê¸°
log_freq = max_ep_len * 2
save_model_freq = int(2e4)

action_std = None                   

update_timestep = max_ep_len * 10   # ì—…ë°ì´íŠ¸ ì‹œê¸°
K_epochs = 40                       # PPO ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ëª‡ ë²ˆ í•˜ëƒ.
eps_clip = 0.2
gamma = 0.99

lr_actor = 3e-4
lr_critic = 1e-3

random_seed = 0

# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨

print("training environment name : " + env_name)

#env = gym.make('gridworld-v0')

# state space dimensionğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ğŸŒ¸ìˆ˜ì • ì‚¬í•­
state_dim = 9*16 # ì§€í›ˆì´ê°€ ë§Œë“  Env.
#state_dim = 16*16 #ë‚´ê°€ ë§Œë“  Env.


# action space dimension
if has_continuous_action_space:
    action_dim = env.action_space.shape[0]  # continuous action spaceë‘ 
else:
    action_dim = 5
    #action_dim = env.action_space.n         # discrete action spaceë‘ shape í‘œí˜„ì´ ì„œë¡œ ë‹¤ë¦„.


######################ğŸ”¥ğŸ”¥ logging(ë¡œê¹…) ğŸ”¥ğŸ”¥######################

#### log files for multiple runs are NOT overwritten

log_dir = "PPO_logs"
os.makedirs(log_dir, exist_ok=True)         # PPO_logsë¼ëŠ” í´ë” ìƒì„±.

log_dir = log_dir + '/' + env_name + '/'
os.makedirs(log_dir, exist_ok=True)         # PPO_logs/CartPole-v1/ë¼ëŠ” í´ë” ìƒì„±.


#### get number of log files in log directory
run_num = 0
current_num_files = next(os.walk(log_dir))[2] #â“â“â“â“â“â“â“â“â“â“â“â“â“â“â“â“
run_num = len(current_num_files)


#### ë§¤ë²ˆ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ csv í˜•ì‹ì˜ log íŒŒì¼ ìƒì„±í•  ê±°ì„.
log_f_name = log_dir + '/PPO_' + env_name + "_log_" + str(run_num) + ".csv"   # csv íŒŒì¼

print("current logging run number for " + env_name + " : ", run_num)
print("logging at : " + log_f_name)

#####################################################


################### checkpointing ###################

run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder

directory = "PPO_preTrained"
os.makedirs(directory, exist_ok=True)   # PPO_preTrainedë¼ëŠ” í´ë” ìƒì„±.

directory = directory + '/' + env_name + '/'
os.makedirs(directory, exist_ok=True)   # PPO_preTrained/CartPole-v1/ë¼ëŠ” í´ë” ìƒì„±.

# ì˜ í•™ìŠµëœ PPO ëª¨ë¸ì„ ì €ì¥í•  ê±°ì„.
checkpoint_path = directory + "PPO_{}_{}_{}.pth".format(env_name, random_seed, run_num_pretrained)
print("save checkpoint path : " + checkpoint_path)

#####################################################


############# ê°ì¢… hyperparameters ì¶œë ¥ #############

print("--------------------------------------------------------------------------------------------")

print("max training timesteps : ", max_training_timesteps)
print("max timesteps per episode : ", max_ep_len)

print("model saving frequency : " + str(save_model_freq) + " timesteps")
print("log frequency : " + str(log_freq) + " timesteps")
print("printing average reward over episodes in last : " + str(print_freq) + " timesteps")

print("--------------------------------------------------------------------------------------------")

print("state space dimension : ", state_dim)
print("action space dimension : ", action_dim)

print("--------------------------------------------------------------------------------------------")

#ğŸ”¥ Continuous Action Space  ---->> ì¹´íŠ¸í´ì´ë¼ ë¬´ì‹œ
if has_continuous_action_space:  
    print("Initializing a continuous action space policy")
    print("--------------------------------------------------------------------------------------------")
    print("starting std of action distribution : ", action_std)
    print("decay rate of std of action distribution : ", action_std_decay_rate)
    print("minimum std of action distribution : ", min_action_std)
    print("decay frequency of std of action distribution : " + str(action_std_decay_freq) + " timesteps")

#ğŸ”¥ Discrete Action Space
else:
    print("Initializing a discrete action space policy") # ê°ì¢… ì„¤ì •ë“¤ ì¶œë ¥.

print("--------------------------------------------------------------------------------------------")

print("PPO update frequency : " + str(update_timestep) + " timesteps") 
print("PPO K epochs : ", K_epochs)
print("PPO epsilon clip : ", eps_clip)
print("discount factor (gamma) : ", gamma)

print("--------------------------------------------------------------------------------------------")

print("optimizer learning rate actor : ", lr_actor)
print("optimizer learning rate critic : ", lr_critic)

if random_seed:
    print("--------------------------------------------------------------------------------------------")
    print("setting random seed to ", random_seed)
    torch.manual_seed(random_seed)
    env.seed(random_seed)
    np.random.seed(random_seed)

#####################################################

print("============================================================================================")

################# training procedure ################

# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥initialize a PPO agentğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)
#                                                                                           initial action_stdì¸ë° ì¹´íŠ¸í´ì´ë¼ì„œ Noneì„.

# track total training time
start_time = datetime.now().replace(microsecond=0) # microsecond ë‹¨ìœ„ëŠ” ì•ˆ ë³´ê² ìŒ.
print("Started training at (GMT) : ", start_time)

print("============================================================================================")


# logging fileì€ csv í˜•ì‹ì˜ íŒŒì¼ì„.
log_f = open(log_f_name, "w+")              # log_f_nameìœ¼ë¡œ ëœ csv íŒŒì¼ì´ ìƒì„±ë¨.
log_f.write('episode, timestep, reward\n')  # ì´ëŸ° ê²ƒë“¤ì„ csv íŒŒì¼ì— ê¸°ë¡.


# printing and logging variables
print_running_reward = 0
print_running_episodes = 0

log_running_reward = 0
log_running_episodes = 0

time_step = 0
i_episode = 0


# ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨


# ğŸ”¥ppo_agent = PPO()

reward_box = []

while time_step <= max_training_timesteps:   # ì„¤ì •ëœ ìµœëŒ€ timestep ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ê³„ì† ëŒë¦¼.
    
    state = env.reset()         # initial State ì¤Œ.
    state = np.ravel(state)

    current_ep_reward = 0

    for t in range(1, max_ep_len+1):  # ê²œ íšŸìˆ˜(ì—í”¼ì†Œë“œ)

        #env.render()
        
        #                                             Environmentì— í•  action êµ¬í•˜ê¸°.
        action = ppo_agent.select_action(state)     # Action  --->> ì—¬ê¸°ì— state, action, log_prob ì €ì¥ ê³¼ì •ë„ ìˆìŒ.
        #state, reward, done, _ = env.step(action)   # State_prime, Reward, Done
        #state = np.ravel(state)


        env_info = env.step(action)

        next_state = env_info[0]
        next_state = np.ravel(next_state)

        reward = env_info[1] - 1 # 1.0 //// 
        #print(reward)

        done = env_info[2]



        
        # saving reward and is_terminals
        ppo_agent.buffer.rewards.append(reward)     # R ì €ì¥.
        ppo_agent.buffer.is_terminals.append(done)  # done ì €ì¥.
        
        time_step +=1
        current_ep_reward += reward     # í˜„ì¬ ì—í”¼ì†Œë“œì˜ ì´ ë³´ìƒ

        #ğŸ”¥íŠ¹ì • ë•Œë§ˆë‹¤ PPO ëª¨ë¸ ì—…ë°ì´íŠ¸ğŸ”¥   -------->> ğŸ”¥ìœ„ì—ì„œ ê³„ì† bufferì— ìŒ“ê¸°ë§Œ í•˜ê³ , ì—…ë°ì´íŠ¸ëŠ” íŠ¹ì • ë•Œë§Œ í•¨.ğŸ”¥
        #                                               ì—¬ê¸°ì„œ K_epoch Loop ëŒì•„ê°.
        if time_step % update_timestep == 0:
            ppo_agent.update()

        # if continuous action space; then decay action std of ouput action distribution
        if has_continuous_action_space and time_step % action_std_decay_freq == 0:
            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)  

        #ğŸ”¥íŠ¹ì • ë•Œë§ˆë‹¤ ê°’ë“¤ logging
        if time_step % log_freq == 0:

            # log average reward till last episode
            log_avg_reward = log_running_reward / log_running_episodes
            log_avg_reward = round(log_avg_reward, 4)

            log_f.write('{},{},{}\n'.format(i_episode, time_step, log_avg_reward)) # csv íŒŒì¼ì— ê¸°ë¡â•â•
            log_f.flush()

            log_running_reward = 0
            log_running_episodes = 0

        #ğŸ”¥íŠ¹ì • ë•Œë§ˆë‹¤ average reward ì¶œë ¥
        if time_step % print_freq == 0:

            # print average reward till last episode
            print_avg_reward = print_running_reward / print_running_episodes
            print_avg_reward = round(print_avg_reward, 2)

            print("Episode : {} \t\t Timestep : {} \t\t Average Reward : {}".format(i_episode, time_step, print_avg_reward))

            print_running_reward = 0
            print_running_episodes = 0
            
        #ğŸ”¥ íŠ¹ì • ë•Œë§ˆë‹¤ model weights ì €ì¥
        if time_step % save_model_freq == 0:
            print("--------------------------------------------------------------------------------------------")
            print("saving model at : " + checkpoint_path)
            ppo_agent.save(checkpoint_path)
            print("model saved")
            print("Elapsed Time  : ", datetime.now().replace(microsecond=0) - start_time)
            print("--------------------------------------------------------------------------------------------")
            
        #ğŸ”¥ ê²œ ëë‚˜ë©´ ì—í”¼ì†Œë“œ ì¢…ë£Œ
        if done:
            break

    print_running_reward += current_ep_reward
    print_running_episodes += 1

    reward_box.append(current_ep_reward) # plotting rewards/episode ìš©ë„

    log_running_reward += current_ep_reward
    log_running_episodes += 1

    i_episode += 1


log_f.close()
env.close()


# ëª¨ë“  ê³¼ì • ë‹¤ ëë‚˜ê³  ë‚˜ì˜¬ ë©˜íŠ¸
print("============================================================================================")
end_time = datetime.now().replace(microsecond=0)  # microsecond ë‹¨ìœ„ëŠ” ì•ˆ ë³´ê² ìŒ.
print("Started training at (GMT) : ", start_time)
print("Finished training at (GMT) : ", end_time)
print("Total training time  : ", end_time - start_time)  # ì „ì²´ ê³¼ì • ëª‡ ë¶„ ê±¸ë ¸ëƒ
print("============================================================================================")


# plotting rewards/episode
fig = plt.figure(figsize=(12, 8)) 
plt.plot(reward_box)
plt.xlabel('Episode')
plt.ylabel('Reward')
plt.title('PPO_Rewards/Episode')
plt.grid()
plt.show()

